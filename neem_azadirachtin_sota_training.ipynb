{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binaypanda/Fruit-In-Sight/blob/main/neem_azadirachtin_sota_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zntOqRXsoEN5"
      },
      "source": [
        "# GoogLeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iabfMFboEN5"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK5-cMDXoEN5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHDLcHLLoEN6"
      },
      "outputs": [],
      "source": [
        "class train(Dataset):\n",
        "    def __init__(self, train_csv_file,root_dir, transform=None):\n",
        "        self.train_annotations = pd.read_csv(train_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.train_annotations.iloc[index, 0])\n",
        "        y_label = int(self.train_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        # for greyscale images\n",
        "        if len(image.shape) == 2:\n",
        "            # Duplicate single channel across all three channels\n",
        "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "class test(Dataset):\n",
        "    def __init__(self, test_csv_file, root_dir, transform=None):\n",
        "        self.test_annotations = pd.read_csv(test_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.test_annotations.iloc[index, 0])\n",
        "        y_label = int(self.test_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "class val(Dataset):\n",
        "    def __init__(self, val_csv_file, root_dir, transform=None):\n",
        "        self.val_annotations = pd.read_csv(val_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.val_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.val_annotations.iloc[index, 0])\n",
        "        y_label = int(self.val_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPzPoCqNoEN6"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameters\n",
        "in_channel = 3\n",
        "learning_rate = 1e-3\n",
        "batch_size = 2\n",
        "num_epochs = 100\n",
        "\n",
        "#transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.Resize((640,640))\n",
        "])\n",
        "\n",
        "train_dataset = train(train_csv_file='/home/neem/a_train.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "test_dataset = test(test_csv_file='/home/neem/a_test.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "val_dataset = val(val_csv_file='/home/neem/a_val.csv', root_dir='/home/neem/a_dataset', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWNvkKlKoEN6"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUUE73U5oEN6"
      },
      "outputs": [],
      "source": [
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFXHpelqoEN6"
      },
      "outputs": [],
      "source": [
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxio8GoMoEN7"
      },
      "outputs": [],
      "source": [
        "#train_set, test_set, val_set = torch.utils.data.random_split(dataset, [2834, 354, 354], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "iIW2kjPXoEN7"
      },
      "outputs": [],
      "source": [
        "for items in train_loader:\n",
        "    print(items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HCNiM0DoEN7"
      },
      "outputs": [],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtEYemMqoEN7"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "model = torchvision.models.googlenet(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01XEM6nboEN7"
      },
      "outputs": [],
      "source": [
        "#Train network\n",
        "t_loss = []\n",
        "v_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    #data to cuda if possible\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #forward\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss = sum(losses)/len(losses)\n",
        "  #print(f'Training loss at epoch {epoch} is {train_loss}')\n",
        "\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x, y = x.to(device=device), y.to(device=device)\n",
        "      out = model(x)\n",
        "      loss += criterion(out, y)\n",
        "  val_loss = loss/len(val_loader)\n",
        "  print(f'Training and Validation loss at epoch {epoch} is {train_loss} and {val_loss}')\n",
        "  t_loss.append(train_loss)\n",
        "  v_loss.append(val_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWUdBGm9oEN7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt1 = plt.figure(figsize=(25,15))\n",
        "\n",
        "x_axis = []\n",
        "for i in range(num_epochs):\n",
        "  x_axis.append(i)\n",
        "plt.plot(x_axis, t_loss, 'g', label = 'training loss')\n",
        "plt.plot(x_axis, v_loss, 'r', label = 'validation loss')\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.title('training loss vs validation loss for GoogleNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pzkA64CoEN8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "  Y = np.array([0])\n",
        "  Y_pred = np.array([0])\n",
        "  f1 = 0\n",
        "  i=0\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      Y = np.append(Y,y.cpu())\n",
        "      i += 1\n",
        "      print('Y after appendingt ', i, ' times : ', Y)\n",
        "      scores = model(x)\n",
        "      _, predictions = scores.max(1)\n",
        "      print(list(predictions))\n",
        "      Y_pred = np.append(Y_pred, predictions.cpu())\n",
        "      print('Y_pred after appending: ', Y_pred)\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples = predictions.size(0)\n",
        "    print(f'got accuracy {float(num_correct)/float(num_samples*100)}')\n",
        "  Y = torch.IntTensor(Y)\n",
        "  print('Y is : ', Y)\n",
        "  Y_pred = torch.IntTensor(Y_pred)\n",
        "  print('Y_pred is: ', Y_pred)\n",
        "  f1 = f1_score(Y, Y_pred)\n",
        "  #print('f1 score: ', f1)\n",
        "  #print('precision, recall, f1 score are (ignore None): ', precision_recall_fscore_support(Y, Y_pred, average = 'binary'))\n",
        "  print(classification_report(Y, Y_pred))\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unrNV6mooEN8"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/home/neem/weights/weights_googlenet.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JUTVx61FoEN8"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on val set:')\n",
        "check_accuracy(val_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BaPunl_eoEN8"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on test set:')\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Pif2K404oEN8"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on training set:')\n",
        "check_accuracy(train_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnhKHI7VoEOA"
      },
      "source": [
        "# Inception v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuGjkj6QoEOA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWyjfRhkoEOA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmiDUCoXoEOA"
      },
      "outputs": [],
      "source": [
        "class train(Dataset):\n",
        "    def __init__(self, train_csv_file,root_dir, transform=None):\n",
        "        self.train_annotations = pd.read_csv(train_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.train_annotations.iloc[index, 0])\n",
        "        y_label = int(self.train_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        # for greyscale images\n",
        "        if len(image.shape) == 2:\n",
        "            # Duplicate single channel across all three channels\n",
        "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "class test(Dataset):\n",
        "    def __init__(self, test_csv_file, root_dir, transform=None):\n",
        "        self.test_annotations = pd.read_csv(test_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.test_annotations.iloc[index, 0])\n",
        "        y_label = int(self.test_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "class val(Dataset):\n",
        "    def __init__(self, val_csv_file, root_dir, transform=None):\n",
        "        self.val_annotations = pd.read_csv(val_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.val_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.val_annotations.iloc[index, 0])\n",
        "        y_label = int(self.val_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap5GXp-MoEOB"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameters\n",
        "in_channel = 3\n",
        "learning_rate = 1e-3\n",
        "batch_size = 2\n",
        "num_epochs = 100\n",
        "\n",
        "#transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.Resize((640,640))\n",
        "])\n",
        "\n",
        "train_dataset = train(train_csv_file='/home/neem/a_train.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "test_dataset = test(test_csv_file='/home/neem/a_test.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "val_dataset = val(val_csv_file='/home/neem/a_val.csv', root_dir='/home/neem/a_dataset', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAiH8FtfoEOB"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvQEOriEoEOB"
      },
      "outputs": [],
      "source": [
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36cjv9jRoEOB"
      },
      "outputs": [],
      "source": [
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpuRIALkoEOB"
      },
      "outputs": [],
      "source": [
        "#train_set, test_set, val_set = torch.utils.data.random_split(dataset, [2834, 354, 354], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_kBTgYvooEOB"
      },
      "outputs": [],
      "source": [
        "for items in train_loader:\n",
        "    print(items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmU8yJljoEOC"
      },
      "outputs": [],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQDkETZ9oEOC"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "model = torchvision.models.inception_v3(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qem8JEemoEOC"
      },
      "outputs": [],
      "source": [
        "#Train network\n",
        "t_loss = []\n",
        "v_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    #data to cuda if possible\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #forward\n",
        "    scores, y = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  train_loss = sum(losses)/len(losses)\n",
        "  #print(f'Training loss at epoch {epoch} is {train_loss}')\n",
        "\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x, y = x.to(device=device), y.to(device=device)\n",
        "      out = model(x)\n",
        "      loss += criterion(out, y)\n",
        "  val_loss = loss/len(val_loader)\n",
        "  print(f'Training and Validation loss at epoch {epoch} is {train_loss} and {val_loss}')\n",
        "  t_loss.append(train_loss)\n",
        "  v_loss.append(val_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVpmzutLoEOC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt1 = plt.figure(figsize=(25,15))\n",
        "\n",
        "x_axis = []\n",
        "for i in range(num_epochs):\n",
        "  x_axis.append(i)\n",
        "plt.plot(x_axis, t_loss, 'g', label = 'training loss')\n",
        "plt.plot(x_axis, v_loss, 'r', label = 'validation loss')\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.title('training loss vs validation loss for InceptionNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrnpTqMWoEOC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "  Y = np.array([0])\n",
        "  Y_pred = np.array([0])\n",
        "  f1 = 0\n",
        "  i=0\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      Y = np.append(Y,y.cpu())\n",
        "      i += 1\n",
        "      print('Y after appendingt ', i, ' times : ', Y)\n",
        "      scores = model(x)\n",
        "      _, predictions = scores.max(1)\n",
        "      print(list(predictions))\n",
        "      Y_pred = np.append(Y_pred, predictions.cpu())\n",
        "      print('Y_pred after appending: ', Y_pred)\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples = predictions.size(0)\n",
        "    print(f'got accuracy {float(num_correct)/float(num_samples*100)}')\n",
        "  Y = torch.IntTensor(Y)\n",
        "  print('Y is : ', Y)\n",
        "  Y_pred = torch.IntTensor(Y_pred)\n",
        "  print('Y_pred is: ', Y_pred)\n",
        "  f1 = f1_score(Y, Y_pred)\n",
        "  #print('f1 score: ', f1)\n",
        "  #print('precision, recall, f1 score are (ignore None): ', precision_recall_fscore_support(Y, Y_pred, average = 'binary'))\n",
        "  print(classification_report(Y, Y_pred))\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq1gy97roEOC"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/home/neem/weights/weights_Inceptionv3.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9rWUksuEoEOC"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on val set:')\n",
        "check_accuracy(val_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3zV07FrioEOD"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on test set:')\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EhTR35M8oEOD"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on training set:')\n",
        "check_accuracy(train_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atCcQtT0oEN9"
      },
      "source": [
        "# EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vASyCfQfoEN9"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxd9JvEdoEN9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8Zdlt45oEN9"
      },
      "outputs": [],
      "source": [
        "class train(Dataset):\n",
        "    def __init__(self, train_csv_file,root_dir, transform=None):\n",
        "        self.train_annotations = pd.read_csv(train_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.train_annotations.iloc[index, 0])\n",
        "        y_label = int(self.train_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        # for greyscale images\n",
        "        if len(image.shape) == 2:\n",
        "            # Duplicate single channel across all three channels\n",
        "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "class test(Dataset):\n",
        "    def __init__(self, test_csv_file, root_dir, transform=None):\n",
        "        self.test_annotations = pd.read_csv(test_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.test_annotations.iloc[index, 0])\n",
        "        y_label = int(self.test_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "class val(Dataset):\n",
        "    def __init__(self, val_csv_file, root_dir, transform=None):\n",
        "        self.val_annotations = pd.read_csv(val_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.val_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.val_annotations.iloc[index, 0])\n",
        "        y_label = int(self.val_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgvPlBFOoEN-"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameters\n",
        "in_channel = 3\n",
        "learning_rate = 1e-3\n",
        "batch_size = 2\n",
        "num_epochs = 100\n",
        "\n",
        "#transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.Resize((640,640))\n",
        "])\n",
        "\n",
        "train_dataset = train(train_csv_file='/home/neem/a_train.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "test_dataset = test(test_csv_file='/home/neem/a_test.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "val_dataset = val(val_csv_file='/home/neem/a_val.csv', root_dir='/home/neem/a_dataset', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4GEbjAhoEN-"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCPL33KYoEN-"
      },
      "outputs": [],
      "source": [
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-2qQqowoEN-"
      },
      "outputs": [],
      "source": [
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENMUmHTMoEN-"
      },
      "outputs": [],
      "source": [
        "#train_set, test_set, val_set = torch.utils.data.random_split(dataset, [2834, 354, 354], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TaLneVdfoEN-"
      },
      "outputs": [],
      "source": [
        "for items in train_loader:\n",
        "    print(items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clKgLV0yoEN_"
      },
      "outputs": [],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfOmpxEcoEN_"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60oXfsDQoEN_"
      },
      "outputs": [],
      "source": [
        "#Train network\n",
        "t_loss = []\n",
        "v_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    #data to cuda if possible\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #forward\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss = sum(losses)/len(losses)\n",
        "  #print(f'Training loss at epoch {epoch} is {train_loss}')\n",
        "\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x, y = x.to(device=device), y.to(device=device)\n",
        "      out = model(x)\n",
        "      loss += criterion(out, y)\n",
        "  val_loss = loss/len(val_loader)\n",
        "  print(f'Training and Validation loss at epoch {epoch} is {train_loss} and {val_loss}')\n",
        "  t_loss.append(train_loss)\n",
        "  v_loss.append(val_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HE82JPOoEN_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt1 = plt.figure(figsize=(25,15))\n",
        "\n",
        "x_axis = []\n",
        "for i in range(num_epochs):\n",
        "  x_axis.append(i)\n",
        "plt.plot(x_axis, t_loss, 'g', label = 'training loss')\n",
        "plt.plot(x_axis, v_loss, 'r', label = 'validation loss')\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.title('training loss vs validation loss for EfficientNetB0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeESQXL4oEN_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "  Y = np.array([0])\n",
        "  Y_pred = np.array([0])\n",
        "  f1 = 0\n",
        "  i=0\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      Y = np.append(Y,y.cpu())\n",
        "      i += 1\n",
        "      print('Y after appendingt ', i, ' times : ', Y)\n",
        "      scores = model(x)\n",
        "      _, predictions = scores.max(1)\n",
        "      print(list(predictions))\n",
        "      Y_pred = np.append(Y_pred, predictions.cpu())\n",
        "      print('Y_pred after appending: ', Y_pred)\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples = predictions.size(0)\n",
        "    print(f'got accuracy {float(num_correct)/float(num_samples*100)}')\n",
        "  Y = torch.IntTensor(Y)\n",
        "  print('Y is : ', Y)\n",
        "  Y_pred = torch.IntTensor(Y_pred)\n",
        "  print('Y_pred is: ', Y_pred)\n",
        "  f1 = f1_score(Y, Y_pred)\n",
        "  #print('f1 score: ', f1)\n",
        "  #print('precision, recall, f1 score are (ignore None): ', precision_recall_fscore_support(Y, Y_pred, average = 'binary'))\n",
        "  print(classification_report(Y, Y_pred))\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D940VfOVoEN_"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/home/neem/weights/weights_EfficientNetB0.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TRychFl7oEN_"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on val set:')\n",
        "check_accuracy(val_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4BPFzF_2oEOA"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on test set:')\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PL8aHpcboEOA"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on training set:')\n",
        "check_accuracy(train_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdqiNa74tU6m"
      },
      "source": [
        "# Resnext_50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOrD-2P-2MKA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPoLGdDAtzc1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loL0ONrEoEN3"
      },
      "outputs": [],
      "source": [
        "class train(Dataset):\n",
        "    def __init__(self, train_csv_file,root_dir, transform=None):\n",
        "        self.train_annotations = pd.read_csv(train_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.train_annotations.iloc[index, 0])\n",
        "        y_label = int(self.train_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        # for greyscale images\n",
        "        if len(image.shape) == 2:\n",
        "            # Duplicate single channel across all three channels\n",
        "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "class test(Dataset):\n",
        "    def __init__(self, test_csv_file, root_dir, transform=None):\n",
        "        self.test_annotations = pd.read_csv(test_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.test_annotations.iloc[index, 0])\n",
        "        y_label = int(self.test_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "class val(Dataset):\n",
        "    def __init__(self, val_csv_file, root_dir, transform=None):\n",
        "        self.val_annotations = pd.read_csv(val_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.val_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.val_annotations.iloc[index, 0])\n",
        "        y_label = int(self.val_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8lK8D5QOeNh"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameters\n",
        "in_channel = 3\n",
        "learning_rate = 1e-3\n",
        "batch_size = 2\n",
        "num_epochs = 100\n",
        "\n",
        "#transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.Resize((640,640))\n",
        "])\n",
        "\n",
        "train_dataset = train(train_csv_file='/home/neem/a_train.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "test_dataset = test(test_csv_file='/home/neem/a_test.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "val_dataset = val(val_csv_file='/home/neem/a_val.csv', root_dir='/home/neem/a_dataset', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq3HupOtoEN3"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwH4GjNaoEN3"
      },
      "outputs": [],
      "source": [
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvf0Zo3OoEN3"
      },
      "outputs": [],
      "source": [
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iscwdPA4BFD8"
      },
      "outputs": [],
      "source": [
        "#train_set, test_set, val_set = torch.utils.data.random_split(dataset, [2834, 354, 354], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pywtuQuc4sY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for items in train_loader:\n",
        "    print(items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KlLCZSndCcB"
      },
      "outputs": [],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZKcXj9R-IlT"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "model = torchvision.models.resnext50_32x4d(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apib0_CSOoIY"
      },
      "outputs": [],
      "source": [
        "#Train network\n",
        "t_loss = []\n",
        "v_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    #data to cuda if possible\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #forward\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss = sum(losses)/len(losses)\n",
        "    #print(f'Training loss at epoch {epoch} is {train_loss}')\n",
        "\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x, y = x.to(device=device), y.to(device=device)\n",
        "      out = model(x)\n",
        "      loss += criterion(out, y)\n",
        "  val_loss = loss/len(val_loader)\n",
        "  print(f'Training and Validation loss at epoch {epoch} is {train_loss} and {val_loss}')\n",
        "  t_loss.append(train_loss)\n",
        "  v_loss.append(val_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO1JHP9FCfd9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt1 = plt.figure(figsize=(25,15))\n",
        "\n",
        "x_axis = []\n",
        "for i in range(num_epochs):\n",
        "  x_axis.append(i)\n",
        "plt.plot(x_axis, t_loss, 'g', label = 'training loss')\n",
        "plt.plot(x_axis, v_loss, 'r', label = 'validation loss')\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.title('training loss vs validation loss for Resnext_50')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkPgzxhCOrI1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "  Y = np.array([0])\n",
        "  Y_pred = np.array([0])\n",
        "  f1 = 0\n",
        "  i=0\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      Y = np.append(Y,y.cpu())\n",
        "      i += 1\n",
        "      print('Y after appendingt ', i, ' times : ', Y)\n",
        "      scores = model(x)\n",
        "      _, predictions = scores.max(1)\n",
        "      print(list(predictions))\n",
        "      Y_pred = np.append(Y_pred, predictions.cpu())\n",
        "      print('Y_pred after appending: ', Y_pred)\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples = predictions.size(0)\n",
        "    print(f'got accuracy {float(num_correct)/float(num_samples*100)}')\n",
        "  Y = torch.IntTensor(Y)\n",
        "  print('Y is : ', Y)\n",
        "  Y_pred = torch.IntTensor(Y_pred)\n",
        "  print('Y_pred is: ', Y_pred)\n",
        "  f1 = f1_score(Y, Y_pred)\n",
        "  #print('f1 score: ', f1)\n",
        "  #print('precision, recall, f1 score are (ignore None): ', precision_recall_fscore_support(Y, Y_pred, average = 'binary'))\n",
        "  print(classification_report(Y, Y_pred))\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzo0WoyhDT_U"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/home/neem/weights/weights_Resnext_50.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KRe6Nyzn70e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on val set:')\n",
        "check_accuracy(val_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3rwjXMDOtqR",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on test set:')\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsNsAHv7OxT7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on training set:')\n",
        "check_accuracy(train_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4vIdsD8oEOD"
      },
      "source": [
        "# Resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nebTnkqVoEOD"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l1vqWe-oEOD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGHyfQLaoEOD"
      },
      "outputs": [],
      "source": [
        "class train(Dataset):\n",
        "    def __init__(self, train_csv_file,root_dir, transform=None):\n",
        "        self.train_annotations = pd.read_csv(train_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.train_annotations.iloc[index, 0])\n",
        "        y_label = int(self.train_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        # for greyscale images\n",
        "        if len(image.shape) == 2:\n",
        "            # Duplicate single channel across all three channels\n",
        "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "class test(Dataset):\n",
        "    def __init__(self, test_csv_file, root_dir, transform=None):\n",
        "        self.test_annotations = pd.read_csv(test_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.test_annotations.iloc[index, 0])\n",
        "        y_label = int(self.test_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "class val(Dataset):\n",
        "    def __init__(self, val_csv_file, root_dir, transform=None):\n",
        "        self.val_annotations = pd.read_csv(val_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.val_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.val_annotations.iloc[index, 0])\n",
        "        y_label = int(self.val_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecePq6ZZoEOE"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameters\n",
        "in_channel = 3\n",
        "learning_rate = 1e-3\n",
        "batch_size = 2\n",
        "num_epochs = 100\n",
        "\n",
        "#transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.Resize((640,640))\n",
        "])\n",
        "\n",
        "train_dataset = train(train_csv_file='/home/neem/a_train.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "test_dataset = test(test_csv_file='/home/neem/a_test.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "val_dataset = val(val_csv_file='/home/neem/a_val.csv', root_dir='/home/neem/a_dataset', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2Pxs9agoEOE"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6abBTJwToEOE"
      },
      "outputs": [],
      "source": [
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPtHz9w0oEOE"
      },
      "outputs": [],
      "source": [
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3h9bQJnoEOE"
      },
      "outputs": [],
      "source": [
        "#train_set, test_set, val_set = torch.utils.data.random_split(dataset, [2834, 354, 354], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JU9imutDoEOE"
      },
      "outputs": [],
      "source": [
        "for items in train_loader:\n",
        "    print(items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-y8xcStoEOE"
      },
      "outputs": [],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyTmRUNIoEOF"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWsQNBFSoEOF"
      },
      "outputs": [],
      "source": [
        "#Train network\n",
        "t_loss = []\n",
        "v_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    #data to cuda if possible\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #forward\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss = sum(losses)/len(losses)\n",
        "  #print(f'Training loss at epoch {epoch} is {train_loss}')\n",
        "\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x, y = x.to(device=device), y.to(device=device)reached\n",
        "      out = model(x)\n",
        "      loss += criterion(out, y)\n",
        "  val_loss = loss/len(val_loader)\n",
        "  print(f'Training and Validation loss at epoch {epoch} is {train_loss} and {val_loss}')\n",
        "  t_loss.append(train_loss)\n",
        "  v_loss.append(val_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Zd6pbToEOF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt1 = plt.figure(figsize=(25,15))\n",
        "\n",
        "x_axis = []\n",
        "for i in range(num_epochs):\n",
        "  x_axis.append(i)\n",
        "plt.plot(x_axis, t_loss, 'g', label = 'training loss')\n",
        "plt.plot(x_axis, v_loss, 'r', label = 'validation loss')\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.title('training loss vs validation loss for Resnet18')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3emkcZZoEOF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "  Y = np.array([0])\n",
        "  Y_pred = np.array([0])\n",
        "  f1 = 0\n",
        "  i=0\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      Y = np.append(Y,y.cpu())\n",
        "      i += 1\n",
        "      print('Y after appendingt ', i, ' times : ', Y)\n",
        "      scores = model(x)\n",
        "      _, predictions = scores.max(1)\n",
        "      print(list(predictions))\n",
        "      Y_pred = np.append(Y_pred, predictions.cpu())\n",
        "      print('Y_pred after appending: ', Y_pred)\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples = predictions.size(0)\n",
        "    print(f'got accuracy {float(num_correct)/float(num_samples*100)}')\n",
        "  Y = torch.IntTensor(Y)\n",
        "  print('Y is : ', Y)\n",
        "  Y_pred = torch.IntTensor(Y_pred)\n",
        "  print('Y_pred is: ', Y_pred)\n",
        "  f1 = f1_score(Y, Y_pred)\n",
        "  #print('f1 score: ', f1)\n",
        "  #print('precision, recall, f1 score are (ignore None): ', precision_recall_fscore_support(Y, Y_pred, average = 'binary'))\n",
        "  print(classification_report(Y, Y_pred))\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h18z1fEoEOF"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/home/neem/weights/weights_Resnet18.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zcoGcAh5oEOF"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on val set:')\n",
        "check_accuracy(val_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OwKUtw0xoEOF"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on test set:')\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nJxBIW3XoEOF"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on training set:')\n",
        "check_accuracy(train_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX4H7lajoEOG"
      },
      "source": [
        "# SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WMbI3V7oEOG"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JllQoccGoEOG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN1wP1UToEOG"
      },
      "outputs": [],
      "source": [
        "class train(Dataset):\n",
        "    def __init__(self, train_csv_file,root_dir, transform=None):\n",
        "        self.train_annotations = pd.read_csv(train_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.train_annotations.iloc[index, 0])\n",
        "        y_label = int(self.train_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        # for greyscale images\n",
        "        if len(image.shape) == 2:\n",
        "            # Duplicate single channel across all three channels\n",
        "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "\n",
        "class test(Dataset):\n",
        "    def __init__(self, test_csv_file, root_dir, transform=None):\n",
        "        self.test_annotations = pd.read_csv(test_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.test_annotations.iloc[index, 0])\n",
        "        y_label = int(self.test_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)\n",
        "\n",
        "class val(Dataset):\n",
        "    def __init__(self, val_csv_file, root_dir, transform=None):\n",
        "        self.val_annotations = pd.read_csv(val_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = Dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.val_annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # load a regular image\n",
        "        image_path = os.path.join(self.root_dir, self.val_annotations.iloc[index, 0])\n",
        "        y_label = int(self.val_annotations.iloc[index, 1])\n",
        "\n",
        "        image = io.imread(image_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return (image, y_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VLltKlKoEOG"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameters\n",
        "in_channel = 3\n",
        "learning_rate = 1e-3\n",
        "batch_size = 2\n",
        "num_epochs = 100\n",
        "\n",
        "#transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.Resize((640,640))\n",
        "])\n",
        "\n",
        "train_dataset = train(train_csv_file='/home/neem/a_train.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "test_dataset = test(test_csv_file='/home/neem/a_test.csv', root_dir='/home/neem/a_dataset', transform=transform)\n",
        "\n",
        "val_dataset = val(val_csv_file='/home/neem/a_val.csv', root_dir='/home/neem/a_dataset', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoT7YlRvoEOG"
      },
      "outputs": [],
      "source": [
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeRqBQlJoEOH"
      },
      "outputs": [],
      "source": [
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXH9VW2YoEOH"
      },
      "outputs": [],
      "source": [
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOfjq7xVoEOH"
      },
      "outputs": [],
      "source": [
        "#train_set, test_set, val_set = torch.utils.data.random_split(dataset, [2834, 354, 354], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Bt5CkGMjoEOH"
      },
      "outputs": [],
      "source": [
        "for items in train_loader:\n",
        "    print(items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2s7imyuoEOH"
      },
      "outputs": [],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9CsDmjroEOH"
      },
      "outputs": [],
      "source": [
        "#model\n",
        "model = torchvision.models.squeezenet1_0(pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15B-sIo5oEOH"
      },
      "outputs": [],
      "source": [
        "#Train network\n",
        "t_loss = []\n",
        "v_loss = []\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    #data to cuda if possible\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #forward\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  train_loss = sum(losses)/len(losses)\n",
        "  #print(f'Training loss at epoch {epoch} is {train_loss}')\n",
        "\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      x, y = x.to(device=device), y.to(device=device)\n",
        "      out = model(x)\n",
        "      loss += criterion(out, y)\n",
        "  val_loss = loss/len(val_loader)\n",
        "  print(f'Training and Validation loss at epoch {epoch} is {train_loss} and {val_loss}')\n",
        "  t_loss.append(train_loss)\n",
        "  v_loss.append(val_loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAHHr8NjoEOI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt1 = plt.figure(figsize=(25,15))\n",
        "\n",
        "x_axis = []\n",
        "for i in range(num_epochs):\n",
        "  x_axis.append(i)\n",
        "plt.plot(x_axis, t_loss, 'g', label = 'training loss')\n",
        "plt.plot(x_axis, v_loss, 'r', label = 'validation loss')\n",
        "plt.legend(['training loss', 'validation loss'])\n",
        "plt.title('training loss vs validation loss for SqueezeNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm7Pf303oEOI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "  Y = np.array([0])\n",
        "  Y_pred = np.array([0])\n",
        "  f1 = 0\n",
        "  i=0\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      Y = np.append(Y,y.cpu())\n",
        "      i += 1\n",
        "      print('Y after appendingt ', i, ' times : ', Y)\n",
        "      scores = model(x)\n",
        "      _, predictions = scores.max(1)\n",
        "      print(list(predictions))\n",
        "      Y_pred = np.append(Y_pred, predictions.cpu())\n",
        "      print('Y_pred after appending: ', Y_pred)\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples = predictions.size(0)\n",
        "    print(f'got accuracy {float(num_correct)/float(num_samples*100)}')\n",
        "  Y = torch.IntTensor(Y)\n",
        "  print('Y is : ', Y)\n",
        "  Y_pred = torch.IntTensor(Y_pred)\n",
        "  print('Y_pred is: ', Y_pred)\n",
        "  f1 = f1_score(Y, Y_pred)\n",
        "  #print('f1 score: ', f1)\n",
        "  #print('precision, recall, f1 score are (ignore None): ', precision_recall_fscore_support(Y, Y_pred, average = 'binary'))\n",
        "  print(classification_report(Y, Y_pred))\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy4xfWdRoEOI"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/home/neem/weights/weights_SqueezeNet.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-QlCRZOioEOI"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on val set:')\n",
        "check_accuracy(val_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0mKq_1OhoEOI"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on test set:')\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RuK7pd59oEOI"
      },
      "outputs": [],
      "source": [
        "print('Checking accuracy on training set:')\n",
        "check_accuracy(train_loader, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}